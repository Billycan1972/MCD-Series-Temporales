{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "results_train_test = []\n",
    "predictions_test = []\n",
    "\n",
    "\n",
    "# Lista para almacenar dataframes de resultados\n",
    "resultados = []\n",
    "\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    \n",
    "    tscv = TimeSeriesSplit(gap=2, n_splits=5)\n",
    "    train_sets = []\n",
    "    test_sets = []\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(df)):\n",
    "        # Elimina el índice adicional, dejando solo uno\n",
    "        train_data = df.iloc[train_index]\n",
    "        test_data = df.iloc[test_index]\n",
    "\n",
    "        train_sets.append(train_data)\n",
    "        test_sets.append(test_data)\n",
    "\n",
    "\n",
    "    # Asigna los conjuntos de entrenamiento y prueba\n",
    "    train1, test1 = train_sets[0], test_sets[0]\n",
    "    train2, test2 = train_sets[1], test_sets[1]\n",
    "    train3, test3 = train_sets[2], test_sets[2]\n",
    "    train4, test4 = train_sets[3], test_sets[3]\n",
    "    train5, test5 = train_sets[4], test_sets[4]\n",
    "    train = [train1,train2,train3,train4,train5]\n",
    "    test = [test1,test2,test3,test4,test5]\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "\n",
    "    for f in range(4):\n",
    "        # Entrenamiento del modelo ARIMA\n",
    "        model = auto_arima(train[f], trace=True, suppress_warnings=True)\n",
    "        pred = model.predict(len(test[f]))\n",
    "        MSE=round(mean_squared_error(test[f], pred),2)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        MAE = mean_absolute_error(test[f], pred)\n",
    "        MAPE = np.mean(np.abs((test[f] - pred.mean()) / test[f])) * 100\n",
    "\n",
    "        \n",
    "        \n",
    "        # Guardar métricas para cada fold\n",
    "        mse_scores.append(MSE)\n",
    "        rmse_scores.append(RMSE)\n",
    "        mae_scores.append(MAE)\n",
    "        mape_scores.append(MAPE)\n",
    "        \n",
    "    # Crear DataFrame de resultados para el dataframe actual\n",
    "    resultados_df = pd.DataFrame({\n",
    "        'DataFrame': f'df{i}_autoarima',\n",
    "        'Fold': range(1, 6),\n",
    "        'MSE': mse_scores,\n",
    "        'RMSE': rmse_scores,\n",
    "        'MAE': mae_scores,\n",
    "        'MAPE': mape_scores\n",
    "    })\n",
    "\n",
    "    # Agregar métricas promedio\n",
    "    resultados_df.loc['Promedio'] = [\n",
    "        f'df{i}_autoarima', \n",
    "        'Promedio',\n",
    "        np.mean(mse_scores),\n",
    "        np.mean(rmse_scores),\n",
    "        np.mean(mae_scores),\n",
    "        np.mean(mape_scores)\n",
    "    ]\n",
    "\n",
    "    # Agregar el DataFrame de resultados a la lista de resultados\n",
    "    resultados.append(resultados_df)\n",
    "\n",
    "# Concatenar todos los resultados en un único DataFrame\n",
    "resultados_final = pd.concat(resultados, ignore_index=True)\n",
    "print(resultados_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "results_train_test = []\n",
    "predictions_test = []\n",
    "\n",
    "\n",
    "# Lista para almacenar dataframes de resultados\n",
    "resultados = []\n",
    "\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    \n",
    "    tscv = TimeSeriesSplit(gap=2, n_splits=5)\n",
    "    train_sets = []\n",
    "    test_sets = []\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(df)):\n",
    "        # Elimina el índice adicional, dejando solo uno\n",
    "        train_data = df.iloc[train_index]\n",
    "        test_data = df.iloc[test_index]\n",
    "\n",
    "        train_sets.append(train_data)\n",
    "        test_sets.append(test_data)\n",
    "\n",
    "\n",
    "    # Asigna los conjuntos de entrenamiento y prueba\n",
    "    train1, test1 = train_sets[0], test_sets[0]\n",
    "    train2, test2 = train_sets[1], test_sets[1]\n",
    "    train3, test3 = train_sets[2], test_sets[2]\n",
    "    train4, test4 = train_sets[3], test_sets[3]\n",
    "    train5, test5 = train_sets[4], test_sets[4]\n",
    "    train = [train1,train2,train3,train4,train5]\n",
    "    test = [test1,test2,test3,test4,test5]\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "\n",
    "    for f in range(4):\n",
    "        # Entrenamiento del modelo ARIMA\n",
    "        model = ExponentialSmoothing(train[f],trend=\"add\",seasonal='add', seasonal_periods=21).fit()\n",
    "        pred = model.predict(len(test[f]))\n",
    "        MSE=round(mean_squared_error(test[f], pred),2)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        MAE = mean_absolute_error(test[f], pred)\n",
    "        MAPE = np.mean(np.abs((test[f] - pred.mean()) / test[f])) * 100\n",
    "\n",
    "        \n",
    "        \n",
    "        # Guardar métricas para cada fold\n",
    "        mse_scores.append(MSE)\n",
    "        rmse_scores.append(RMSE)\n",
    "        mae_scores.append(MAE)\n",
    "        mape_scores.append(MAPE)\n",
    "        \n",
    "    # Crear DataFrame de resultados para el dataframe actual\n",
    "    resultados_df = pd.DataFrame({\n",
    "        'DataFrame': f'df{i}_ExponentialSmoothing',\n",
    "        'Fold': range(1, 6),\n",
    "        'MSE': mse_scores,\n",
    "        'RMSE': rmse_scores,\n",
    "        'MAE': mae_scores,\n",
    "        'MAPE': mape_scores\n",
    "    })\n",
    "\n",
    "    # Agregar métricas promedio\n",
    "    resultados_df.loc['Promedio'] = [\n",
    "        f'df{i}_ExponentialSmoothing', \n",
    "        'Promedio',\n",
    "        np.mean(mse_scores),\n",
    "        np.mean(rmse_scores),\n",
    "        np.mean(mae_scores),\n",
    "        np.mean(mape_scores)\n",
    "    ]\n",
    "\n",
    "    # Agregar el DataFrame de resultados a la lista de resultados\n",
    "    resultados.append(resultados_df)\n",
    "\n",
    "# Concatenar todos los resultados en un único DataFrame\n",
    "resultados_final = pd.concat(resultados, ignore_index=True)\n",
    "print(resultados_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "results_train_test = []\n",
    "predictions_test = []\n",
    "\n",
    "\n",
    "# Lista para almacenar dataframes de resultados\n",
    "resultados = []\n",
    "\n",
    "\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    \n",
    "    tscv = TimeSeriesSplit(gap=2, n_splits=5)\n",
    "    train_sets = []\n",
    "    test_sets = []\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(df)):\n",
    "        # Elimina el índice adicional, dejando solo uno\n",
    "        train_data = df.iloc[train_index]\n",
    "        test_data = df.iloc[test_index]\n",
    "\n",
    "        train_sets.append(train_data)\n",
    "        test_sets.append(test_data)\n",
    "\n",
    "\n",
    "    # Asigna los conjuntos de entrenamiento y prueba\n",
    "    train1, test1 = train_sets[0], test_sets[0]\n",
    "    train2, test2 = train_sets[1], test_sets[1]\n",
    "    train3, test3 = train_sets[2], test_sets[2]\n",
    "    train4, test4 = train_sets[3], test_sets[3]\n",
    "    train5, test5 = train_sets[4], test_sets[4]\n",
    "    train = [train1,train2,train3,train4,train5]\n",
    "    test = [test1,test2,test3,test4,test5]\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "\n",
    "    for f in range(4):\n",
    "        # Entrenamiento del modelo ARIMA\n",
    "        model = SimpleExpSmoothing(train[f]).fit()\n",
    "        pred = model.predict(len(test[f]))\n",
    "        MSE=round(mean_squared_error(test[f], pred),2)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        MAE = mean_absolute_error(test[f], pred)\n",
    "        MAPE = np.mean(np.abs((test[f] - pred.mean()) / test[f])) * 100\n",
    "\n",
    "        \n",
    "        \n",
    "        # Guardar métricas para cada fold\n",
    "        mse_scores.append(MSE)\n",
    "        rmse_scores.append(RMSE)\n",
    "        mae_scores.append(MAE)\n",
    "        mape_scores.append(MAPE)\n",
    "        \n",
    "    # Crear DataFrame de resultados para el dataframe actual\n",
    "    resultados_df = pd.DataFrame({\n",
    "        'DataFrame': f'df{i}_SimpleExpSmoothing',\n",
    "        'Fold': range(1, 6),\n",
    "        'MSE': mse_scores,\n",
    "        'RMSE': rmse_scores,\n",
    "        'MAE': mae_scores,\n",
    "        'MAPE': mape_scores\n",
    "    })\n",
    "\n",
    "    # Agregar métricas promedio\n",
    "    resultados_df.loc['Promedio'] = [\n",
    "        f'df{i}_SimpleExpSmoothing', \n",
    "        'Promedio',\n",
    "        np.mean(mse_scores),\n",
    "        np.mean(rmse_scores),\n",
    "        np.mean(mae_scores),\n",
    "        np.mean(mape_scores)\n",
    "    ]\n",
    "\n",
    "    # Agregar el DataFrame de resultados a la lista de resultados\n",
    "    resultados.append(resultados_df)\n",
    "\n",
    "# Concatenar todos los resultados en un único DataFrame\n",
    "resultados_final = pd.concat(resultados, ignore_index=True)\n",
    "print(resultados_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "results_train_test = []\n",
    "predictions_test = []\n",
    "\n",
    "\n",
    "# Lista para almacenar dataframes de resultados\n",
    "resultados = []\n",
    "\n",
    "\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    \n",
    "    tscv = TimeSeriesSplit(gap=2, n_splits=5)\n",
    "    train_sets = []\n",
    "    test_sets = []\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(df)):\n",
    "        # Elimina el índice adicional, dejando solo uno\n",
    "        train_data = df.iloc[train_index]\n",
    "        test_data = df.iloc[test_index]\n",
    "\n",
    "        train_sets.append(train_data)\n",
    "        test_sets.append(test_data)\n",
    "\n",
    "\n",
    "    # Asigna los conjuntos de entrenamiento y prueba\n",
    "    train1, test1 = train_sets[0], test_sets[0]\n",
    "    train2, test2 = train_sets[1], test_sets[1]\n",
    "    train3, test3 = train_sets[2], test_sets[2]\n",
    "    train4, test4 = train_sets[3], test_sets[3]\n",
    "    train5, test5 = train_sets[4], test_sets[4]\n",
    "    train = [train1,train2,train3,train4,train5]\n",
    "    test = [test1,test2,test3,test4,test5]\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, mape_scores = [], [], [], []\n",
    "\n",
    "    for f in range(4):\n",
    "        # Entrenamiento del modelo ARIMA\n",
    "        if i in [0,1]: \n",
    "            m = 21\n",
    "        else :\n",
    "            m = 6\n",
    "        model = auto_arima(train[f],\n",
    "                        start_p=0,\n",
    "                        start_q=0,\n",
    "                        max_p=8,\n",
    "                        max_q=8,\n",
    "                        m=m, # segun gpt recomiendas m=21 ya que 21 dias laborales en un mes.\n",
    "                        start_P=0,\n",
    "                        seasonal=True,\n",
    "                        d=None,\n",
    "                        D=None,\n",
    "                        trace=True,\n",
    "                        error_action='ignore',\n",
    "                        suppress_warnings=True,\n",
    "                        stepwise=True)\n",
    "        \n",
    "        pred = model.predict(len(test[f]))\n",
    "        MSE=round(mean_squared_error(test[f], pred),2)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        MAE = mean_absolute_error(test[f], pred)\n",
    "        MAPE = np.mean(np.abs((test[f] - pred.mean()) / test[f])) * 100\n",
    "\n",
    "        \n",
    "        \n",
    "        # Guardar métricas para cada fold\n",
    "        mse_scores.append(MSE)\n",
    "        rmse_scores.append(RMSE)\n",
    "        mae_scores.append(MAE)\n",
    "        mape_scores.append(MAPE)\n",
    "        \n",
    "    # Crear DataFrame de resultados para el dataframe actual\n",
    "    resultados_df = pd.DataFrame({\n",
    "        'DataFrame': f'df{i}_SimpleExpSmoothing',\n",
    "        'Fold': range(1, 6),\n",
    "        'MSE': mse_scores,\n",
    "        'RMSE': rmse_scores,\n",
    "        'MAE': mae_scores,\n",
    "        'MAPE': mape_scores\n",
    "    })\n",
    "\n",
    "    # Agregar métricas promedio\n",
    "    resultados_df.loc['Promedio'] = [\n",
    "        f'df{i}_SimpleExpSmoothing', \n",
    "        'Promedio',\n",
    "        np.mean(mse_scores),\n",
    "        np.mean(rmse_scores),\n",
    "        np.mean(mae_scores),\n",
    "        np.mean(mape_scores)\n",
    "    ]\n",
    "\n",
    "    # Agregar el DataFrame de resultados a la lista de resultados\n",
    "    resultados.append(resultados_df)\n",
    "\n",
    "# Concatenar todos los resultados en un único DataFrame\n",
    "resultados_final = pd.concat(resultados, ignore_index=True)\n",
    "print(resultados_final)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
